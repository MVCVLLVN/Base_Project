Классическая нейронная сеть по классификации изображений
В данном проекте я реализовал свою первую нерйонную сеть, предназначенную для распознавания кошек и собак с картинкоке. Данный проект реализован в процессе обучения на курсе "Код Будущего" для ознакомления с принципами работы нейронных сетей. Так же статистическим методом была выявлена оптимальная настройка неронной сети для получения наибольшего процента правильных ответов.
Определим некоторые понятия
Входной слой
Во входном слое все нейроны принимают от входных данных сигнал и имеют множество связей со скрытым(выходным) слоем, куда на каждый нейрон последующего слоя отдается выходной сигнал.

Скрытый слой(слои)
В скрытом слое(ях) каждый нейрон принимает суммарный сигнал от каждого нейрона входного слоя, пропускает его через функцию сглаживания(про нее позднее) и отдает на каждый нейрон следующего слоя.

Сверточный слой.
Первый и основной слой в сверточной нейронной сети. Он состоит из свертки и активационной функции. Свертка применяется к входным данным, чтобы получить выходные значения на основе определенного набора фильтров. Фильтры представляют собой матрицу весов, которые обучаются на этапе обучения. Активационная функция добавляет нелинейность в выходные значения, что позволяет сети лучше обрабатывать нелинейные связи в данных.

Выходной слой.
В выходном слое каждый нейрон принимает сигнал от каждого нейрона скрытого слоя, пропускает его через функцию сглаживания, и мы получаем результат работы нейронной сети.

Связи нейронов
Каждый нейрон имеет связь, которая имеет весовой коэффициент, абстарактно обозначающий силу связи с между нейронами. Именно улчешение значения весов или уравновешивание и является основной идеей обучения нейронной сети.

Процесс обучения нейронной сети
Обучение нейронной сети заключается в том, что мы постоянно уравновешиваем наши весовые коэффициенты, чтобы получить оптимальные значения. Но как мы их должны уравновешивать? Мы подаем нейронной сети на вход данные, сеть пропускает их через себя и выдает что-то, вероятнее всего мало похожее на ответ. Тогда мы показываем ей то, каким должен был быть ответ, сравниваем его с полученными данными и получаем значение ошибки. И так получаем значение ошибки на каждом слое нейронов. Поиск оптимальных значений весов называется градиентным спуском, то есть поиск локального минимума. С помощью величины ошибки и градиентного спуска мы получаем то, на сколько надо изменить значения весовых коэффициентов, чтобы оказаться ближе к минимуму. Весь этот метод имеет общее название - обратное распространение ошибки. Формула, по которой вычисляется значение насколько надо изменить весовые коэффициенты с учетом ошибки выглядит следующим образом:

∆w_jk - величина изменения k нейрона в j слое.
α - коэффициент обучения (или шаг градиентного спуска).
E_k - ошибка на k неройроне.
O_k - входной сигнал k нейрона (суммарный).
сигмоида(O_k) - выходной сигнал k нейрона.
O_j - вектор входных сигналов.